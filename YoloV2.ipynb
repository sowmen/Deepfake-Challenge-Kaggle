{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YoloV2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_mobilenetv2_224_075_detector(path):\n",
        "    input_tensor = Input(shape=(224, 224, 3))\n",
        "    output_tensor = MobileNetV2(weights=None, include_top=False, input_tensor=input_tensor, alpha=0.75).output\n",
        "    output_tensor = ZeroPadding2D()(output_tensor)\n",
        "    output_tensor = Conv2D(kernel_size=(3, 3), filters=5)(output_tensor)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    model.load_weights(path)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "WARNING:tensorflow:From C:\\Users\\sowme\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n"
        }
      ],
      "source": [
        "mobilenetv2 = load_mobilenetv2_224_075_detector(\"facedetection-mobilenetv2-size224-alpha0.75.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converts A:B aspect rate to B:A\n",
        "def transpose_shots(shots):\n",
        "    return [(shot[1], shot[0], shot[3], shot[2], shot[4]) for shot in shots]\n",
        "\n",
        "#That constant describe pieces for 16:9 images\n",
        "SHOTS = {\n",
        "    # fast less accurate\n",
        "    '2-16/9' : {\n",
        "        'aspect_ratio' : 16/9,\n",
        "        'shots' : [\n",
        "             (0, 0, 9/16, 1, 1),\n",
        "             (7/16, 0, 9/16, 1, 1)\n",
        "        ]\n",
        "    },\n",
        "    # slower more accurate\n",
        "    '10-16/9' : {\n",
        "        'aspect_ratio' : 16/9,\n",
        "        'shots' : [\n",
        "             (0, 0, 9/16, 1, 1),\n",
        "             (7/16, 0, 9/16, 1, 1),\n",
        "             (0, 0, 5/16, 5/9, 0.5),\n",
        "             (0, 4/9, 5/16, 5/9, 0.5),\n",
        "             (11/48, 0, 5/16, 5/9, 0.5),\n",
        "             (11/48, 4/9, 5/16, 5/9, 0.5),\n",
        "             (22/48, 0, 5/16, 5/9, 0.5),\n",
        "             (22/48, 4/9, 5/16, 5/9, 0.5),\n",
        "             (11/16, 0, 5/16, 5/9, 0.5),\n",
        "             (11/16, 4/9, 5/16, 5/9, 0.5),\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 9:16 respectively\n",
        "SHOTS_T = {\n",
        "    '2-9/16' : {\n",
        "        'aspect_ratio' : 9/16,\n",
        "        'shots' : transpose_shots(SHOTS['2-16/9']['shots'])\n",
        "    },\n",
        "    '10-9/16' : {\n",
        "        'aspect_ratio' : 9/16,\n",
        "        'shots' : transpose_shots(SHOTS['10-16/9']['shots'])\n",
        "    }\n",
        "}\n",
        "\n",
        "def r(x):\n",
        "    return int(round(x))\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (np.exp(-x) + 1)\n",
        "\n",
        "def non_max_suppression(boxes, p, iou_threshold):\n",
        "    if len(boxes) == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    indexes = np.argsort(p)\n",
        "    true_boxes_indexes = []\n",
        "\n",
        "    while len(indexes) > 0:\n",
        "        true_boxes_indexes.append(indexes[-1])\n",
        "\n",
        "        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n",
        "        iou = intersection / ((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]) + (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]) - intersection)\n",
        "\n",
        "        indexes = np.delete(indexes, -1)\n",
        "        indexes = np.delete(indexes, np.where(iou >= iou_threshold)[0])\n",
        "\n",
        "    return boxes[true_boxes_indexes]\n",
        "\n",
        "def union_suppression(boxes, threshold):\n",
        "    if len(boxes) == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    indexes = np.argsort((x2 - x1) * (y2 - y1))\n",
        "    result_boxes = []\n",
        "\n",
        "    while len(indexes) > 0:\n",
        "        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n",
        "        min_s = np.minimum((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]), (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]))\n",
        "        ioms = intersection / (min_s + 1e-9)\n",
        "        neighbours = np.where(ioms >= threshold)[0]\n",
        "        if len(neighbours) > 0:\n",
        "            result_boxes.append([min(np.min(x1[indexes[neighbours]]), x1[indexes[-1]]), min(np.min(y1[indexes[neighbours]]), y1[indexes[-1]]), max(np.max(x2[indexes[neighbours]]), x2[indexes[-1]]), max(np.max(y2[indexes[neighbours]]), y2[indexes[-1]])])\n",
        "        else:\n",
        "            result_boxes.append([x1[indexes[-1]], y1[indexes[-1]], x2[indexes[-1]], y2[indexes[-1]]])\n",
        "\n",
        "        indexes = np.delete(indexes, -1)\n",
        "        indexes = np.delete(indexes, neighbours)\n",
        "\n",
        "    return result_boxes\n",
        "\n",
        "class FaceDetector():\n",
        "    \"\"\"\n",
        "    That's API you can easily use to detect faces\n",
        "    \n",
        "    __init__ parameters:\n",
        "    -------------------------------\n",
        "    model - model to infer\n",
        "    shots - list of aspect ratios that images could be (described earlier)\n",
        "    image_size - model's input size (hardcoded for mobilenetv2)\n",
        "    grids - model's output size (hardcoded for mobilenetv2)\n",
        "    union_threshold - threshold for union of predicted boxes within multiple shots\n",
        "    iou_threshold - IOU threshold for non maximum suppression used to merge YOLO detected boxes for one shot,\n",
        "                    you do need to change this because there are one face per image as I can see from the samples\n",
        "    prob_threshold - probability threshold for YOLO algorithm, you can balance beetween precision and recall using this threshold\n",
        "    \n",
        "    detect parameters:\n",
        "    -------------------------------\n",
        "    frame - (1920, 1080, 3) or (1080, 1920, 3) RGB Image\n",
        "    returns: list of 4 element tuples (left corner x, left corner y, right corner x, right corner y) of detected boxes within [0, 1] range (see box draw code below)\n",
        "    \"\"\"\n",
        "    def __init__(self, model=mobilenetv2, shots=[SHOTS['10-16/9'], SHOTS_T['10-9/16']], image_size=224, grids=7, iou_threshold=0.1, union_threshold=0.1):\n",
        "        self.model = model\n",
        "        self.shots = shots\n",
        "        self.image_size = image_size\n",
        "        self.grids = grids\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.union_threshold = union_threshold\n",
        "        self.prob_threshold = 0.7\n",
        "        \n",
        "    \n",
        "    def detect(self, frame, threshold = 0.7):\n",
        "        original_frame_shape = frame.shape\n",
        "        self.prob_threshold = threshold\n",
        "        aspect_ratio = None\n",
        "        for shot in self.shots:\n",
        "            if abs(frame.shape[1] / frame.shape[0] - shot[\"aspect_ratio\"]) < 1e-9:\n",
        "                aspect_ratio = shot[\"aspect_ratio\"]\n",
        "                shots = shot\n",
        "        \n",
        "        assert aspect_ratio is not None\n",
        "        \n",
        "        c = min(frame.shape[0], frame.shape[1] / aspect_ratio)\n",
        "        slice_h_shift = r((frame.shape[0] - c) / 2)\n",
        "        slice_w_shift = r((frame.shape[1] - c * aspect_ratio) / 2)\n",
        "        if slice_w_shift != 0 and slice_h_shift == 0:\n",
        "            frame = frame[:, slice_w_shift:-slice_w_shift]\n",
        "        elif slice_w_shift == 0 and slice_h_shift != 0:\n",
        "            frame = frame[slice_h_shift:-slice_h_shift, :]\n",
        "\n",
        "        frames = []\n",
        "        for s in shots[\"shots\"]:\n",
        "            frames.append(cv2.resize(frame[r(s[1] * frame.shape[0]):r((s[1] + s[3]) * frame.shape[0]), r(s[0] * frame.shape[1]):r((s[0] + s[2]) * frame.shape[1])], (self.image_size, self.image_size), interpolation=cv2.INTER_NEAREST))\n",
        "        frames = np.array(frames)\n",
        "\n",
        "        predictions = self.model.predict(frames, batch_size=len(frames), verbose=0)\n",
        "\n",
        "        boxes = []\n",
        "        prob = []\n",
        "        shots = shots['shots']\n",
        "        for i in range(len(shots)):\n",
        "            slice_boxes = []\n",
        "            slice_prob = []\n",
        "            for j in range(predictions.shape[1]):\n",
        "                for k in range(predictions.shape[2]):\n",
        "                    p = sigmoid(predictions[i][j][k][4])\n",
        "                    if not(p is None) and p > self.prob_threshold:\n",
        "                        px = sigmoid(predictions[i][j][k][0])\n",
        "                        py = sigmoid(predictions[i][j][k][1])\n",
        "                        pw = min(math.exp(predictions[i][j][k][2] / self.grids), self.grids)\n",
        "                        ph = min(math.exp(predictions[i][j][k][3] / self.grids), self.grids)\n",
        "                        if not(px is None) and not(py is None) and not(pw is None) and not(ph is None) and pw > 1e-9 and ph > 1e-9:\n",
        "                            cx = (px + j) / self.grids\n",
        "                            cy = (py + k) / self.grids\n",
        "                            wx = pw / self.grids\n",
        "                            wy = ph / self.grids\n",
        "                            if wx <= shots[i][4] and wy <= shots[i][4]:\n",
        "                                lx = min(max(cx - wx / 2, 0), 1)\n",
        "                                ly = min(max(cy - wy / 2, 0), 1)\n",
        "                                rx = min(max(cx + wx / 2, 0), 1)\n",
        "                                ry = min(max(cy + wy / 2, 0), 1)\n",
        "\n",
        "                                lx *= shots[i][2]\n",
        "                                ly *= shots[i][3]\n",
        "                                rx *= shots[i][2]\n",
        "                                ry *= shots[i][3]\n",
        "\n",
        "                                lx += shots[i][0]\n",
        "                                ly += shots[i][1]\n",
        "                                rx += shots[i][0]\n",
        "                                ry += shots[i][1]\n",
        "\n",
        "                                slice_boxes.append([lx, ly, rx, ry])\n",
        "                                slice_prob.append(p)\n",
        "\n",
        "            slice_boxes = np.array(slice_boxes)\n",
        "            slice_prob = np.array(slice_prob)\n",
        "\n",
        "            slice_boxes = non_max_suppression(slice_boxes, slice_prob, self.iou_threshold)\n",
        "\n",
        "            for sb in slice_boxes:\n",
        "                boxes.append(sb)\n",
        "\n",
        "\n",
        "        boxes = np.array(boxes)\n",
        "        boxes = union_suppression(boxes, self.union_threshold)\n",
        "\n",
        "        for i in range(len(boxes)):\n",
        "            boxes[i][0] /= original_frame_shape[1] / frame.shape[1]\n",
        "            boxes[i][1] /= original_frame_shape[0] / frame.shape[0]\n",
        "            boxes[i][2] /= original_frame_shape[1] / frame.shape[1]\n",
        "            boxes[i][3] /= original_frame_shape[0] / frame.shape[0]\n",
        "\n",
        "            boxes[i][0] += slice_w_shift / original_frame_shape[1]\n",
        "            boxes[i][1] += slice_h_shift / original_frame_shape[0]\n",
        "            boxes[i][2] += slice_w_shift / original_frame_shape[1]\n",
        "            boxes[i][3] += slice_h_shift / original_frame_shape[0]\n",
        "\n",
        "        return list(boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_boxes_points(boxes, frame_shape):\n",
        "    result = []\n",
        "    for box in boxes:\n",
        "        lx = int(round(box[0] * frame_shape[1]))\n",
        "        ly = int(round(box[1] * frame_shape[0]))\n",
        "        rx = int(round(box[2] * frame_shape[1]))\n",
        "        ry = int(round(box[3] * frame_shape[0]))\n",
        "        result.append((lx, ly, rx, ry))\n",
        "    return result "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "yolo_model = FaceDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_FOLDER = \"F:\\ML\\deepfake-detection-challenge\"\n",
        "TRAIN_FOLDER = 'train_sample_videos'\n",
        "TEST_FOLDER = 'test_videos'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Train samples: 401\nTest samples: 400\n"
        }
      ],
      "source": [
        "print(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_FOLDER)))}\")\n",
        "print(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_list = list(os.listdir(os.path.join(DATA_FOLDER, TRAIN_FOLDER)))\n",
        "test_list = list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "JSON file: metadata.json\n"
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>split</th>\n      <th>original</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>aagfhgtpmv.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>vudstovrck.mp4</td>\n    </tr>\n    <tr>\n      <th>aapnvogymq.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>jdubbvfswz.mp4</td>\n    </tr>\n    <tr>\n      <th>abarnvbtwb.mp4</th>\n      <td>REAL</td>\n      <td>train</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>abofeumbvv.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>atvmxvwyns.mp4</td>\n    </tr>\n    <tr>\n      <th>abqwwspghj.mp4</th>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>qzimuostzz.mp4</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "               label  split        original\naagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\naapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\nabarnvbtwb.mp4  REAL  train            None\nabofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\nabqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_meta_from_json(path):\n",
        "    df = pd.read_json(os.path.join(DATA_FOLDER, path, json_file))\n",
        "    df = df.T\n",
        "    return df\n",
        "\n",
        "json_file = [file for file in train_list if  file.endswith('json')][0]\n",
        "print(f\"JSON file: {json_file}\")\n",
        "\n",
        "meta_train_df = get_meta_from_json(TRAIN_FOLDER)\n",
        "meta_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ECQxcilQt33v"
      },
      "outputs": [],
      "source": [
        "sample_real = \"drive/My Drive/DeepFake_test/asaxgevnnp_REAL.mp4\"\n",
        "sample_fake = \"drive/My Drive/DeepFake_test/akxoopqjqz_FAKE.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def video_to_faces(video_name, input_loc, output_loc):\n",
        "    \"\"\"Function to extract faces from input video file\n",
        "    and save them as separate images in an output directory.\n",
        "    Args:\n",
        "        input_loc: Input video file.\n",
        "        output_loc: Output directory to save the frames.\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Log the time\n",
        "    time_start = time.time()\n",
        "    # Start capturing the feed\n",
        "    cap = cv2.VideoCapture(input_loc)\n",
        "    # Find the number of frames\n",
        "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
        "    print(\"Processing : \" + video_name)\n",
        "    print (\"Number of frames: \", video_length)\n",
        "    count = 0\n",
        "    print (\"Converting video..\\n\")\n",
        "    # Start converting the video\n",
        "    while cap.isOpened():\n",
        "        # Extract the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame_copy = np.array(frame)\n",
        "        yolo_boxes = yolo_model.detect(frame, 0.7)\n",
        "        yb = get_boxes_points(yolo_boxes, frame.shape)\n",
        "\n",
        "        face_count = 0\n",
        "        for b in yb:\n",
        "            lx, ly, rx, ry = b\n",
        "            # x, y, w, h here\n",
        "            # ax.add_patch(Rectangle((lx,ly),rx - lx,ry - ly,linewidth=2,edgecolor='red',facecolor='none'))\n",
        "            img_crop = frame_copy[ly:ry, lx:rx]\n",
        "            img_crop = cv2.cvtColor(img_crop, cv2.COLOR_RGB2BGR)\n",
        "            cv2.imwrite(output_loc + \"/%#05d_%#02d.jpg\" % ((count+1), (face_count+1)), img_crop)\n",
        "            face_count += 1\n",
        "\n",
        "        # Write the results back to output location.\n",
        "        \n",
        "        count = count + 1\n",
        "        # If there are no more frames left\n",
        "        if (count > (video_length-1)):\n",
        "            # Log the time again\n",
        "            time_end = time.time()\n",
        "            # Release the feed\n",
        "            cap.release()\n",
        "            # Print stats\n",
        "            print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
        "            print (\"It took %d seconds for conversion.\\n\\n\" % (time_end-time_start))\n",
        "            break\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Creation of the directory output_faces/dqppxmoqdl.mp4_FAKE failed\nProcessing : dqppxmoqdl.mp4\nNumber of frames:  299\nConverting video..\n\n1\n1\n1\n1\n1\n1\n1\n"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-30-4e63fc282a41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Creation of the directory %s failed\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvideo_to_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dqppxmoqdl.mp4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTRAIN_FOLDER\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"dqppxmoqdl.mp4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-28-6f77c0a94187>\u001b[0m in \u001b[0;36mvideo_to_faces\u001b[1;34m(video_name, input_loc, output_loc)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mframe_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0myolo_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myolo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_boxes_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myolo_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-4-66495329b080>\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, frame, threshold)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "path_out = \"output_faces/\" + \"dqppxmoqdl.mp4\" + \"_\" + meta_train_df.loc[\"dqppxmoqdl.mp4\"].label\n",
        "try:\n",
        "    os.mkdir(path_out)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory %s failed\" % path_out)\n",
        "video_to_faces(\"dqppxmoqdl.mp4\", os.path.join(DATA_FOLDER,TRAIN_FOLDER,\"dqppxmoqdl.mp4\"), os.path.join(os.getcwd(),path_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Processing : dqppxmoqdl.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 210 seconds for conversion.\n\n\nProcessing : dgzklxjmix.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 191 seconds for conversion.\n\n\nProcessing : bourlmzsio.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 189 seconds for conversion.\n\n\nProcessing : cepxysienc.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 188 seconds for conversion.\n\n\nProcessing : bbvgxeczei.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 184 seconds for conversion.\n\n\nProcessing : bdgipnyobr.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 186 seconds for conversion.\n\n\nProcessing : dvumqqhoac.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 207 seconds for conversion.\n\n\nProcessing : bhaaboftbc.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 203 seconds for conversion.\n\n\nProcessing : dhcndnuwta.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 202 seconds for conversion.\n\n\nProcessing : dakqwktlbi.mp4\nNumber of frames:  299\nConverting video..\n\nDone extracting frames.\n299 frames extracted\nIt took 199 seconds for conversion.\n\n\n"
        }
      ],
      "source": [
        "#Select random videos\n",
        "random_train_selection = random.sample(train_list, 10)\n",
        "\n",
        "for video in random_train_selection:\n",
        "    path_out = \"output_faces/\" + video + \"_\" + meta_train_df.loc[video].label\n",
        "    try:\n",
        "        os.mkdir(path_out)\n",
        "    except OSError:\n",
        "        print (\"Creation of the directory %s failed\" % path)\n",
        "\n",
        "    video_to_faces(video, os.path.join(DATA_FOLDER,TRAIN_FOLDER,video), os.path.join(os.getcwd(),path_out))\n"
      ]
    }
  ]
}