{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Inference Kernel Demo\n\nThis is the kernel I’ve used for my recent submissions. It takes about 5-6 hours on the test set, using only CPU. \n\nI’ve provided this kernel because a lot of people have problems making submissions. This method works and has never errored out for me. (Although I haven't tried making a submission using the GPU yet -- so no guarantees there.)\n\nIt uses BlazeFace for face extraction (see also [my BlazeFace kernel](https://www.kaggle.com/humananalog/starter-blazeface-pytorch)) and ResNeXt50 as the classifier model.\n\nWe take the average prediction over 17 frames from each video. (Why 17? Using more frames makes the kernel slower, but doesn't appear to improve the score much. I used an odd number so we don't always land on even frames.)\n\n**Please use this kernel only to learn from...** Included is the checkpoint for a ResNeXt50 model that hasn't really been trained very well yet. I'm sure you can improve on it by training your own model!\n\nYou could use the included trained weights to get yourself an easy top-50 score on the leaderboard (as of 24 Jan 2020) but it’s nicer to use it as a starting point for your own work. :-)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, sys, time\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the test videos"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n\ntest_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\nlen(test_videos)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create helpers"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"cuDNN version:\", torch.backends.cudnn.version())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\nsys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from blazeface import BlazeFace\nfacedet = BlazeFace().to(gpu)\nfacedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\nfacedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n_ = facedet.train(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from helpers.read_video_1 import VideoReader\nfrom helpers.face_extract_1 import FaceExtractor\n\nframes_per_video = 17\n\nvideo_reader = VideoReader()\nvideo_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\nface_extractor = FaceExtractor(video_read_fn, facedet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = 224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.transforms import Normalize\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nnormalize_transform = Normalize(mean, std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n    h, w = img.shape[:2]\n    if w > h:\n        h = h * size // w\n        w = size\n    else:\n        w = w * size // h\n        h = size\n\n    resized = cv2.resize(img, (w, h), interpolation=resample)\n    return resized\n\n\ndef make_square_image(img):\n    h, w = img.shape[:2]\n    size = max(h, w)\n    t = 0\n    b = size - h\n    l = 0\n    r = size - w\n    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\nclass MyResNeXt(models.resnet.ResNet):\n    def __init__(self, training=True):\n        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n                                        layers=[3, 4, 6, 3], \n                                        groups=32, \n                                        width_per_group=4)\n        self.fc = nn.Linear(2048, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/deepfakes-inference-demo/resnext.pth\", map_location=gpu)\n\nmodel = MyResNeXt().to(gpu)\nmodel.load_state_dict(checkpoint)\n_ = model.eval()\n\ndel checkpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/imutils/imutils-0.5.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport skimage\nfrom skimage import data\nfrom skimage.color import rgb2gray\nfrom skimage.color import gray2rgb\nfrom skimage import measure\nfrom skimage.metrics import structural_similarity as ssim\nimport argparse\nimport random\nfrom imutils.video import FPS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def structuralSimilarityIndex(imageA, imageB):\n  dim = (50, 50)\n  A = cv2.resize(imageA, dim, interpolation = cv2.INTER_AREA)\n  B = cv2.resize(imageB, dim, interpolation = cv2.INTER_AREA)\n  grayA = cv2.cvtColor(A, cv2.COLOR_BGR2GRAY) #For SSIM Index -> always conver image to a gray-scale image\n  grayB = cv2.cvtColor(B, cv2.COLOR_BGR2GRAY)\n  ans = ssim(grayA, grayB, full = True)\n  ret = ans[0] #Returns value between -1 and 1\n  ret += 1 #I scaled up the returned value \n  ret /= 2 #By dividing it: i limited the returned value between 0 and 1.\n  return ret # So, now value close to 0 means mismatch, value close to 1 means match.\n\n#print(structuralSimilarityIndex(image3, image3))\n#print(structuralSimilarityIndex(image3, image4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom threading import Thread\nfrom queue import Queue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FileVideoStream:\n    def __init__(self, path, queueSize=128):\n        # initialize the file video stream along with the boolean\n        # used to indicate if the thread should be stopped or not\n        self.stream = cv2.VideoCapture(path)\n        self.stopped = False\n        # initialize the queue used to store frames read from\n        # the video file\n        self.Q = Queue(maxsize=queueSize)\n        \n    def start(self):\n        # start a thread to read frames from the file video stream\n        t = Thread(target=self.update, args=())\n        t.daemon = True\n        t.start()\n        return self\n    \n    def update(self):\n        # keep looping infinitely\n        while True:\n            # if the thread indicator variable is set, stop the\n            # thread\n            if self.stopped:\n                return\n            # otherwise, ensure the queue has room in it\n            if not self.Q.full():\n                # read the next frame from the file\n                (grabbed, frame) = self.stream.read()\n                # if the `grabbed` boolean is `False`, then we have\n                # reached the end of the video file\n                if not grabbed:\n                    self.stop()\n                    return\n                # add the frame to the queue\n                self.Q.put(frame)\n                \n    def read(self):\n        # return next frame in the queue\n        return self.Q.get()\n    \n    def more(self):\n        # return True if there are still frames in the queue\n        return self.Q.qsize() > 0\n    \n    def stop(self):\n        # indicate that the thread should be stopped\n        self.stopped = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_frames(path):\n    \n    capture = cv2.VideoCapture(path)\n    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    frames = []\n    i = 0\n    for frame_idx in range(int(frame_count)):\n        # Get the next frame, but don't decode if we're not using it.\n        ret = capture.grab()\n        if not ret: \n            print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n            \n        if i % 3 == 0:\n            ret, frame = capture.retrieve()\n            if ret == False:\n                break\n#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        i += 1\n    \n    capture.release()\n\n    return frames","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_on_video(video_path, batch_size):\n    try:\n        frames = read_frames(video_path)\n        print(len(frames))\n        \n    except Exception as e:\n        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n\n    return 0.5","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_videos[0])\nstart_time = time.time()\npredict_on_video(os.path.join(test_dir, test_videos[0]), batch_size=frames_per_video)\nend_time = time.time() - start_time\nprint(\"Elapsed : \", end_time)","execution_count":72,"outputs":[{"output_type":"stream","text":"aassnaulhq.mp4\n299\nElapsed :  4.106967449188232\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\n\ndef predict_on_video_set(videos, num_workers):\n    def process_file(i):\n        filename = videos[i]\n        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n        return y_pred\n\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        predictions = ex.map(process_file, range(len(videos)))\n\n    return list(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Speed test\n\nThe leaderboard submission must finish within 9 hours. With 4000 test videos, that is `9*60*60/4000 = 8.1` seconds per video. So if the average time per video is greater than ~8 seconds, the kernel will be too slow!"},{"metadata":{"trusted":true},"cell_type":"code","source":"speed_test = True  # you have to enable this manually","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if speed_test:\n    start_time = time.time()\n    speedtest_videos = test_videos[:5]\n    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n    elapsed = time.time() - start_time\n    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make the submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = predict_on_video_set(test_videos, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\nsubmission_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}