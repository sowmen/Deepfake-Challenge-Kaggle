{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Inference Kernel Demo\n\nThis is the kernel I’ve used for my recent submissions. It takes about 5-6 hours on the test set, using only CPU. \n\nI’ve provided this kernel because a lot of people have problems making submissions. This method works and has never errored out for me. (Although I haven't tried making a submission using the GPU yet -- so no guarantees there.)\n\nIt uses BlazeFace for face extraction (see also [my BlazeFace kernel](https://www.kaggle.com/humananalog/starter-blazeface-pytorch)) and ResNeXt50 as the classifier model.\n\nWe take the average prediction over 17 frames from each video. (Why 17? Using more frames makes the kernel slower, but doesn't appear to improve the score much. I used an odd number so we don't always land on even frames.)\n\n**Please use this kernel only to learn from...** Included is the checkpoint for a ResNeXt50 model that hasn't really been trained very well yet. I'm sure you can improve on it by training your own model!\n\nYou could use the included trained weights to get yourself an easy top-50 score on the leaderboard (as of 24 Jan 2020) but it’s nicer to use it as a starting point for your own work. :-)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, sys, time\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the test videos"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_dir = \"/kaggle/input/deepfake-sample-set\"\n\ntest_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\nlen(test_videos)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"22"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Create helpers"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"cuDNN version:\", torch.backends.cudnn.version())","execution_count":3,"outputs":[{"output_type":"stream","text":"PyTorch version: 1.3.0\nCUDA version: 10.0.130\ncuDNN version: 7603\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\nsys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ---- Blazeface ----"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from blazeface import BlazeFace\n# facedet = BlazeFace().to(gpu)\n# facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n# facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n# _ = facedet.train(False)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from helpers.read_video_1 import VideoReader\n# from helpers.face_extract_1 import FaceExtractor\n\n\n# video_reader = VideoReader()\n# video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n# face_extractor = FaceExtractor(video_read_fn, facedet)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ---------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = 224\nframes_per_video = 50","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.transforms import Normalize\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nnormalize_transform = Normalize(mean, std)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n    h, w = img.shape[:2]\n    if w > h:\n        h = h * size // w\n        w = size\n    else:\n        w = w * size // h\n        h = size\n\n    resized = cv2.resize(img, (w, h), interpolation=resample)\n    return resized\n\n\ndef make_square_image(img):\n    h, w = img.shape[:2]\n    size = max(h, w)\n    t = 0\n    b = size - h\n    l = 0\n    r = size - w\n    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\nclass MyResNeXt(models.resnet.ResNet):\n    def __init__(self, training=True):\n        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n                                        layers=[3, 4, 6, 3], \n                                        groups=32, \n                                        width_per_group=4)\n        self.fc = nn.Linear(2048, 1)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/deepfakes-inference-demo/resnext.pth\", map_location=gpu)\n\nResmodel = MyResNeXt().to(gpu)\nResmodel.load_state_dict(checkpoint)\n_ = Resmodel.eval()\n\ndel checkpoint","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Frame Seperation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport skimage\nfrom skimage import data\nfrom skimage.color import rgb2gray\nfrom skimage.color import gray2rgb\nfrom skimage import measure\nfrom skimage.metrics import structural_similarity as ssim\nimport argparse\nimport random\nimport time","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def structuralSimilarityIndex(imageA, imageB):\n#     dim = (50, 50)\n#     A = cv2.resize(imageA, dim, interpolation = cv2.INTER_AREA)\n#     B = cv2.resize(imageB, dim, interpolation = cv2.INTER_AREA)\n\n#     For SSIM Index -> always conver image to a gray-scale image\n    ans = ssim(imageA, imageB, full = True)\n    ret = ans[0] #Returns value between -1 and 1\n    ret += 1 #I scaled up the returned value \n    ret /= 2 #By dividing it: i limited the returned value between 0 and 1.\n    return ret # So, now value close to 0 means mismatch, value close to 1 means match.\n\n#print(structuralSimilarityIndex(image3, image3))\n#print(structuralSimilarityIndex(image3, image4))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_frames(path):\n    \n    capture = cv2.VideoCapture(path)\n    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    frames = []\n    i = 0\n    for frame_idx in range(int(frame_count)):\n        # Get the next frame, but don't decode if we're not using it.\n        ret = capture.grab()\n        if not ret: \n            print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n            \n        if i % 2 == 0:\n            ret, frame = capture.retrieve()\n            if ret == False:\n                break\n            \n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        i += 1\n    \n    capture.release()\n    \n    print(\"Total frames:   \" + str(len(frames)))\n\n    return frames","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ----- Faster Tensor Reader -----"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !cp /kaggle/input/decord/install.sh . && chmod  +x install.sh && ./install.sh ","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sys.path.insert(0,'/kaggle/working/reader/python')\n\n# from decord import VideoReader\n# from decord import cpu, gpu\n# from decord.bridge import set_bridge\n\n# set_bridge('torch')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def read_frames_faster(path):\n#     video = VideoReader(path, ctx=gpu())\n#     data = video.get_batch(range(len(video)))\n    \n# #     numpy_data = [data.detach().cpu().numpy() for data in tensor_data]\n#     return data","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import torchvision.transforms as transforms\n\n# pil2tensor = transforms.ToTensor()\n# tensor2pil = transforms.ToPILImage()\n\n# def plot_image(tensor):\n#     plt.figure()\n#     # imshow needs a numpy array with the channel dimension\n#     # as the the last dimension so we have to transpose things.\n#     plt.imshow(tensor.cpu().numpy().transpose(1, 2, 0))\n#     plt.show()","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# -----------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_frames(video_path):\n    frames = read_frames(video_path)\n    \n    # Extra\n    ret = []\n    tot = 0\n    for i in range(0, len(frames)):\n        tot += 1\n        if tot % 6 == 0:\n            ret.append(frames[i])\n        if len(ret) == 50:\n            break\n    \n    if len(ret) == 50:\n        return ret\n    else:\n        while len(ret) < 50:\n            rand = random.randrange(0, len(frames))\n            #print(\"Random: \" + str(rand))\n            ret.append(frames[rand])\n        return ret\n    \n    # Extra \n    \n    \n    gray_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) for frame in frames]\n    print(len(frames))\n\n    g = [[] for i in range(len(gray_frames))]\n    mask= []\n    resized_frames = []\n\n    lmt = len(gray_frames)\n    pair = gray_frames[0].shape\n\n    for i in range(lmt + 1):\n        mask.append(0)\n\n        if i == lmt:\n            continue\n        if pair[0] == 1080 and pair[1] == 1920:\n            resized_frames.append(cv2.resize(gray_frames[i], (180, 320), interpolation = cv2.INTER_AREA))\n        else:\n            resized_frames.append(cv2.resize(gray_frames[i], (320, 180), interpolation = cv2.INTER_AREA))\n\n    def union(start_idx, calls):\n        mask[start_idx] = 1\n        curr_img = resized_frames[start_idx]  #Current index is marked as 1\n        g[calls].append(start_idx)\n        for i in range(start_idx + 1, lmt):\n            if mask[i] == 1:\n                continue\n            next_img = (resized_frames[i])\n            if(structuralSimilarityIndex(next_img, curr_img) >= 0.97):\n                mask[i] = 1\n                g[calls].append(i)  #Similar frames are put together\n            else:\n                break\n\n    index = 0 \n    total_sets = 0\n    for i in range(lmt):\n        if mask[i] == 0:\n            union(i, index)\n            total_sets += 1\n            index += 1\n    \n   # print(\"Total disjoint sets:  \" + str(total_sets))\n    \n    final_frames =  []\n    index_taken = [] #\n    visit = [0 for i in range(len(frames))]\n    for i in range(total_sets):\n#             print(\"Rand: \" + str(g[i][0]))\n#         if i >= frames_per_video:\n#            break\n        final_frames.append(frames[g[i][0]])\n        visit[g[i][0]] = 1\n#         index_taken.append(g[i][0])\n\n    while True:\n        if len(final_frames) >= frames_per_video:\n            break\n        rand = random.randrange(0, len(frames))\n        if visit[rand] == 1:\n            continue\n        else:\n            visit[rand] = 1\n            #print(\"Random number -> \" + str(rand))\n            final_frames.append(frames[rand])\n            \n    return final_frames        ","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# YOLOv2 Face Detector"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, Dense, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_mobilenetv2_224_075_detector(path):\n    input_tensor = Input(shape=(224, 224, 3))\n    output_tensor = MobileNetV2(weights=None, include_top=False, input_tensor=input_tensor, alpha=0.75).output\n    output_tensor = ZeroPadding2D()(output_tensor)\n    output_tensor = Conv2D(kernel_size=(3, 3), filters=5)(output_tensor)\n\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    model.load_weights(path)\n    \n    return model","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mobilenetv2 = load_mobilenetv2_224_075_detector(\"/kaggle/input/facedetection-mobilenetv2/facedetection-mobilenetv2-size224-alpha0.75.h5\")","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converts A:B aspect rate to B:A\ndef transpose_shots(shots):\n    return [(shot[1], shot[0], shot[3], shot[2], shot[4]) for shot in shots]\n\n#That constant describe pieces for 16:9 images\nSHOTS = {\n    # fast less accurate\n    '2-16/9' : {\n        'aspect_ratio' : 16/9,\n        'shots' : [\n             (0, 0, 9/16, 1, 1),\n             (7/16, 0, 9/16, 1, 1)\n        ]\n    },\n    # slower more accurate\n    '10-16/9' : {\n        'aspect_ratio' : 16/9,\n        'shots' : [\n             (0, 0, 9/16, 1, 1),\n             (7/16, 0, 9/16, 1, 1),\n             (0, 0, 5/16, 5/9, 0.5),\n             (0, 4/9, 5/16, 5/9, 0.5),\n             (11/48, 0, 5/16, 5/9, 0.5),\n             (11/48, 4/9, 5/16, 5/9, 0.5),\n             (22/48, 0, 5/16, 5/9, 0.5),\n             (22/48, 4/9, 5/16, 5/9, 0.5),\n             (11/16, 0, 5/16, 5/9, 0.5),\n             (11/16, 4/9, 5/16, 5/9, 0.5),\n        ]\n    }\n}\n\n# 9:16 respectively\nSHOTS_T = {\n    '2-9/16' : {\n        'aspect_ratio' : 9/16,\n        'shots' : transpose_shots(SHOTS['2-16/9']['shots'])\n    },\n    '10-9/16' : {\n        'aspect_ratio' : 9/16,\n        'shots' : transpose_shots(SHOTS['10-16/9']['shots'])\n    }\n}\n\ndef r(x):\n    return int(round(x))\n\ndef sigmoid(x):\n    return 1 / (np.exp(-x) + 1)\n\ndef non_max_suppression(boxes, p, iou_threshold):\n    if len(boxes) == 0:\n        return np.array([])\n\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    indexes = np.argsort(p)\n    true_boxes_indexes = []\n\n    while len(indexes) > 0:\n        true_boxes_indexes.append(indexes[-1])\n\n        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n        iou = intersection / ((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]) + (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]) - intersection)\n\n        indexes = np.delete(indexes, -1)\n        indexes = np.delete(indexes, np.where(iou >= iou_threshold)[0])\n\n    return boxes[true_boxes_indexes]\n\ndef union_suppression(boxes, threshold):\n    if len(boxes) == 0:\n        return np.array([])\n\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    indexes = np.argsort((x2 - x1) * (y2 - y1))\n    result_boxes = []\n\n    while len(indexes) > 0:\n        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n        min_s = np.minimum((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]), (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]))\n        ioms = intersection / (min_s + 1e-9)\n        neighbours = np.where(ioms >= threshold)[0]\n        if len(neighbours) > 0:\n            result_boxes.append([min(np.min(x1[indexes[neighbours]]), x1[indexes[-1]]), min(np.min(y1[indexes[neighbours]]), y1[indexes[-1]]), max(np.max(x2[indexes[neighbours]]), x2[indexes[-1]]), max(np.max(y2[indexes[neighbours]]), y2[indexes[-1]])])\n        else:\n            result_boxes.append([x1[indexes[-1]], y1[indexes[-1]], x2[indexes[-1]], y2[indexes[-1]]])\n\n        indexes = np.delete(indexes, -1)\n        indexes = np.delete(indexes, neighbours)\n\n    return result_boxes\n\nclass FaceDetector():\n    \"\"\"\n    That's API you can easily use to detect faces\n    \n    __init__ parameters:\n    -------------------------------\n    model - model to infer\n    shots - list of aspect ratios that images could be (described earlier)\n    image_size - model's input size (hardcoded for mobilenetv2)\n    grids - model's output size (hardcoded for mobilenetv2)\n    union_threshold - threshold for union of predicted boxes within multiple shots\n    iou_threshold - IOU threshold for non maximum suppression used to merge YOLO detected boxes for one shot,\n                    you do need to change this because there are one face per image as I can see from the samples\n    prob_threshold - probability threshold for YOLO algorithm, you can balance beetween precision and recall using this threshold\n    \n    detect parameters:\n    -------------------------------\n    frame - (1920, 1080, 3) or (1080, 1920, 3) RGB Image\n    returns: list of 4 element tuples (left corner x, left corner y, right corner x, right corner y) of detected boxes within [0, 1] range (see box draw code below)\n    \"\"\"\n    def __init__(self, model=mobilenetv2, shots=[SHOTS['10-16/9'], SHOTS_T['10-9/16']], image_size=224, grids=7, iou_threshold=0.1, union_threshold=0.1):\n        self.model = model\n        self.shots = shots\n        self.image_size = image_size\n        self.grids = grids\n        self.iou_threshold = iou_threshold\n        self.union_threshold = union_threshold\n        self.prob_threshold = 0.7\n        \n    \n    def detect(self, frame, threshold = 0.7):\n        original_frame_shape = frame.shape\n        self.prob_threshold = threshold\n        aspect_ratio = None\n        for shot in self.shots:\n            if abs(frame.shape[1] / frame.shape[0] - shot[\"aspect_ratio\"]) < 1e-9:\n                aspect_ratio = shot[\"aspect_ratio\"]\n                shots = shot\n        \n        assert aspect_ratio is not None\n        \n        c = min(frame.shape[0], frame.shape[1] / aspect_ratio)\n        slice_h_shift = r((frame.shape[0] - c) / 2)\n        slice_w_shift = r((frame.shape[1] - c * aspect_ratio) / 2)\n        if slice_w_shift != 0 and slice_h_shift == 0:\n            frame = frame[:, slice_w_shift:-slice_w_shift]\n        elif slice_w_shift == 0 and slice_h_shift != 0:\n            frame = frame[slice_h_shift:-slice_h_shift, :]\n\n        frames = []\n        for s in shots[\"shots\"]:\n            frames.append(cv2.resize(frame[r(s[1] * frame.shape[0]):r((s[1] + s[3]) * frame.shape[0]), r(s[0] * frame.shape[1]):r((s[0] + s[2]) * frame.shape[1])], (self.image_size, self.image_size), interpolation=cv2.INTER_NEAREST))\n        frames = np.array(frames)\n\n        predictions = self.model.predict(frames, batch_size=len(frames), verbose=0)\n\n        boxes = []\n        prob = []\n        shots = shots['shots']\n        for i in range(len(shots)):\n            slice_boxes = []\n            slice_prob = []\n            for j in range(predictions.shape[1]):\n                for k in range(predictions.shape[2]):\n                    p = sigmoid(predictions[i][j][k][4])\n                    if not(p is None) and p > self.prob_threshold:\n                        px = sigmoid(predictions[i][j][k][0])\n                        py = sigmoid(predictions[i][j][k][1])\n                        pw = min(math.exp(predictions[i][j][k][2] / self.grids), self.grids)\n                        ph = min(math.exp(predictions[i][j][k][3] / self.grids), self.grids)\n                        if not(px is None) and not(py is None) and not(pw is None) and not(ph is None) and pw > 1e-9 and ph > 1e-9:\n                            cx = (px + j) / self.grids\n                            cy = (py + k) / self.grids\n                            wx = pw / self.grids\n                            wy = ph / self.grids\n                            if wx <= shots[i][4] and wy <= shots[i][4]:\n                                lx = min(max(cx - wx / 2, 0), 1)\n                                ly = min(max(cy - wy / 2, 0), 1)\n                                rx = min(max(cx + wx / 2, 0), 1)\n                                ry = min(max(cy + wy / 2, 0), 1)\n\n                                lx *= shots[i][2]\n                                ly *= shots[i][3]\n                                rx *= shots[i][2]\n                                ry *= shots[i][3]\n\n                                lx += shots[i][0]\n                                ly += shots[i][1]\n                                rx += shots[i][0]\n                                ry += shots[i][1]\n\n                                slice_boxes.append([lx, ly, rx, ry])\n                                slice_prob.append(p)\n\n            slice_boxes = np.array(slice_boxes)\n            slice_prob = np.array(slice_prob)\n\n            slice_boxes = non_max_suppression(slice_boxes, slice_prob, self.iou_threshold)\n\n            for sb in slice_boxes:\n                boxes.append(sb)\n\n\n        boxes = np.array(boxes)\n        boxes = union_suppression(boxes, self.union_threshold)\n\n        for i in range(len(boxes)):\n            boxes[i][0] /= original_frame_shape[1] / frame.shape[1]\n            boxes[i][1] /= original_frame_shape[0] / frame.shape[0]\n            boxes[i][2] /= original_frame_shape[1] / frame.shape[1]\n            boxes[i][3] /= original_frame_shape[0] / frame.shape[0]\n\n            boxes[i][0] += slice_w_shift / original_frame_shape[1]\n            boxes[i][1] += slice_h_shift / original_frame_shape[0]\n            boxes[i][2] += slice_w_shift / original_frame_shape[1]\n            boxes[i][3] += slice_h_shift / original_frame_shape[0]\n\n        return list(boxes)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_boxes_points(boxes, frame_shape):\n    result = []\n    for box in boxes:\n        lx = int(round(box[0] * frame_shape[1]))\n        ly = int(round(box[1] * frame_shape[0]))\n        rx = int(round(box[2] * frame_shape[1]))\n        ry = int(round(box[3] * frame_shape[0]))\n        result.append((lx, ly, rx, ry))\n    return result","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yolo_model = FaceDetector()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_on_video(video_path, batch_size):\n    try:\n        \n#         start_time = time.time()\n        frames = process_frames(video_path)\n#         print(\"Frame process : \", time.time() - start_time)\n        \n        print(len(frames))\n        \n        faces = []\n        \n#         time2 = time.time()\n        for frame in frames:\n            yolo_boxes = yolo_model.detect(frame, 0.7)\n            yb = get_boxes_points(yolo_boxes, frame.shape)\n            \n            for b in yb:\n                lx, ly, rx, ry = b\n                # x, y, w, h here\n                # ax.add_patch(Rectangle((lx,ly),rx - lx,ry - ly,linewidth=2,edgecolor='red',facecolor='none'))\n                img_crop = frame[ly:ry, lx:rx]\n                faces.append(img_crop)\n#         print(\"Face process : \", time.time() - time2)\n        sample_faces = random.sample(faces, len(faces))\n        if len(sample_faces) > 0:\n            # NOTE: When running on the CPU, the batch size must be fixed\n            # or else memory usage will blow up. (Bug in PyTorch?)\n            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n\n            # If we found any faces, prepare them for the model.\n            n = 0\n            \n            for face in sample_faces:\n                resized_face = isotropically_resize_image(face, input_size)\n                resized_face = make_square_image(resized_face)\n\n                if n < batch_size:\n                    x[n] = resized_face\n                    n += 1\n                else:\n                    print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n                    \n            if n > 0:\n                x = torch.tensor(x, device=gpu).float()\n\n                # Preprocess the images.\n                x = x.permute((0, 3, 1, 2))\n\n                for i in range(len(x)):\n                    x[i] = normalize_transform(x[i] / 255.)\n\n                # Make a prediction, then take the average.\n                with torch.no_grad():\n                    y_pred = Resmodel(x)\n                    y_pred = torch.sigmoid(y_pred.squeeze())\n                    return y_pred[:n].mean().item()\n                \n#         print(\"Total : \", time.time() - start_time)\n        \n    except Exception as e:\n        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n\n    return 0.5","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(test_videos[0])\n\n# # frames = read_frames_faster(os.path.join(test_dir, test_videos[0]))\n# start_time = time.time()\n# out = predict_on_video(os.path.join(test_dir, test_videos[1]), batch_size=frames_per_video)\n# print(\"Total : \", time.time() - start_time)\n# print(out)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\n\ndef predict_on_video_set(videos, num_workers):\n    def process_file(i):\n        filename = videos[i]\n        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n        return y_pred\n\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        predictions = ex.map(process_file, range(len(videos)))\n\n    return list(predictions)","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Speed test\n\nThe leaderboard submission must finish within 9 hours. With 4000 test videos, that is `9*60*60/4000 = 8.1` seconds per video. So if the average time per video is greater than ~8 seconds, the kernel will be too slow!"},{"metadata":{"trusted":true},"cell_type":"code","source":"speed_test = True  # you have to enable this manually","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if speed_test:\n    torch.cuda.empty_cache()\n    start_time = time.time()\n    speedtest_videos = test_videos[:5]\n    predictions = []\n    for video in speedtest_videos:\n        out = predict_on_video(os.path.join(test_dir, video), batch_size=frames_per_video)\n        predictions.append(out)\n    elapsed = time.time() - start_time\n    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))","execution_count":31,"outputs":[{"output_type":"stream","text":"Total frames:   150\n50\nTotal frames:   150\n50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nTotal frames:   150\n50\nTotal frames:   149\n50\nTotal frames:   150\n50\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: overflow encountered in exp\n","name":"stderr"},{"output_type":"stream","text":"Elapsed 25.831847 sec. Average per video: 5.166369 sec.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"[0.8917793035507202,\n 0.4242251515388489,\n 0.5974590182304382,\n 0.9619265794754028,\n 0.7801046371459961]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Make the submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\ntorch.cuda.empty_cache()\nfor video in test_videos:\n    out = predict_on_video(os.path.join(test_dir, video), batch_size=frames_per_video)\n    print(\"Value of out: \" + str(out))\n#     if out >= 0.85:\n#         out = 0.99999999\n#     elif out <= 0.3:\n#         out = 0.01\n    predictions.append(out)\n    print(out)","execution_count":33,"outputs":[{"output_type":"stream","text":"Total frames:   150\n50\nWARNING: have 50 faces but batch size is 50\nValue of out: 0.8809096813201904\n0.8809096813201904\nTotal frames:   150\n50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nValue of out: 0.4680227041244507\n0.4680227041244507\nTotal frames:   150\n50\nValue of out: 0.587143063545227\n0.587143063545227\nTotal frames:   149\n50\nValue of out: 0.9744592308998108\n0.9744592308998108\nTotal frames:   150\n50\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: overflow encountered in exp\n","name":"stderr"},{"output_type":"stream","text":"Value of out: 0.7753207087516785\n0.7753207087516785\nTotal frames:   150\n50\nValue of out: 0.2197372317314148\n0.2197372317314148\nTotal frames:   150\n50\nWARNING: have 50 faces but batch size is 50\nValue of out: 0.7384054660797119\n0.7384054660797119\nTotal frames:   150\n50\nValue of out: 0.5495702624320984\n0.5495702624320984\nTotal frames:   150\n50\nValue of out: 0.8982484340667725\n0.8982484340667725\nTotal frames:   150\n50\nValue of out: 0.02924565225839615\n0.02924565225839615\nTotal frames:   149\n50\nValue of out: 0.21080061793327332\n0.21080061793327332\nTotal frames:   150\n50\nValue of out: 0.0028972518630325794\n0.0028972518630325794\nTotal frames:   150\n50\nValue of out: 0.9911201000213623\n0.9911201000213623\nTotal frames:   150\n50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nValue of out: 0.5702213048934937\n0.5702213048934937\nTotal frames:   150\n50\nValue of out: 0.01403127983212471\n0.01403127983212471\nTotal frames:   150\n50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nWARNING: have 50 faces but batch size is 50\nValue of out: 0.38661062717437744\n0.38661062717437744\nTotal frames:   150\n50\nValue of out: 0.9951736330986023\n0.9951736330986023\nTotal frames:   150\n50\nValue of out: 0.008298704400658607\n0.008298704400658607\nTotal frames:   150\n50\nValue of out: 0.28407278656959534\n0.28407278656959534\nTotal frames:   150\n50\nValue of out: 0.9772639274597168\n0.9772639274597168\nTotal frames:   150\n50\nValue of out: 0.1421143263578415\n0.1421143263578415\nTotal frames:   150\n50\nValue of out: 0.008869168348610401\n0.008869168348610401\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","execution_count":34,"outputs":[{"output_type":"stream","text":"               filename     label\n0   aagfhgtpmv_FAKE.mp4  0.880910\n1   aapnvogymq_FAKE.mp4  0.468023\n2   abofeumbvv_FAKE.mp4  0.587143\n3   acxwigylke_FAKE.mp4  0.974459\n4   akxoopqjqz_FAKE.mp4  0.775321\n5   anpuvshzoo_REAL.mp4  0.219737\n6   apogckdfrz_FAKE.mp4  0.738405\n7   asaxgevnnp_REAL.mp4  0.549570\n8   asdpeebotb_FAKE.mp4  0.898248\n9   atkdltyyen_REAL.mp4  0.029246\n10  beboztfcme_REAL.mp4  0.210801\n11  bilnggbxgu_REAL.mp4  0.002897\n12  blpchvmhxx_FAKE.mp4  0.991120\n13  cffffbcywc_FAKE.mp4  0.570221\n14  dbnygxtwek_REAL.mp4  0.014031\n15  dhcndnuwta_REAL.mp4  0.386611\n16  diomeixhrg_FAKE.mp4  0.995174\n17  drcyabprvt_REAL.mp4  0.008299\n18  dsjbknkujw_REAL.mp4  0.284073\n19  ecuvtoltue_FAKE.mp4  0.977264\n20  ehtdtkmmli_REAL.mp4  0.142114\n21  eqnoqyfquo_REAL.mp4  0.008869\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"              filename     label\n0  aagfhgtpmv_FAKE.mp4  0.880910\n1  aapnvogymq_FAKE.mp4  0.468023\n2  abofeumbvv_FAKE.mp4  0.587143\n3  acxwigylke_FAKE.mp4  0.974459\n4  akxoopqjqz_FAKE.mp4  0.775321","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aagfhgtpmv_FAKE.mp4</td>\n      <td>0.880910</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aapnvogymq_FAKE.mp4</td>\n      <td>0.468023</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abofeumbvv_FAKE.mp4</td>\n      <td>0.587143</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>acxwigylke_FAKE.mp4</td>\n      <td>0.974459</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>akxoopqjqz_FAKE.mp4</td>\n      <td>0.775321</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ncsvData = pd.read_csv(\"../input/deepfake-sample-set/csvData.csv\")\ncsvData","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"              Name  Label\n0   aagfhgtpmv.mp4      1\n1   aapnvogymq.mp4      1\n2   abofeumbvv.mp4      1\n3   acxwigylke.mp4      1\n4   akxoopqjqz.mp4      1\n5   anpuvshzoo.mp4      0\n6   apogckdfrz.mp4      1\n7   asaxgevnnp.mp4      0\n8   asdpeebotb.mp4      1\n9   atkdltyyen.mp4      0\n10  beboztfcme.mp4      0\n11  bilnggbxgu.mp4      0\n12  blpchvmhxx.mp4      1\n13  cffffbcywc.mp4      1\n14  dbnygxtwek.mp4      0\n15  dhcndnuwta.mp4      0\n16  diomeixhrg.mp4      1\n17  drcyabprvt.mp4      0\n18  dsjbknkujw.mp4      0\n19  ecuvtoltue.mp4      1\n20  ehtdtkmmli.mp4      0\n21  eqnoqyfquo.mp4      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aagfhgtpmv.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aapnvogymq.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abofeumbvv.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>acxwigylke.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>akxoopqjqz.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>anpuvshzoo.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>apogckdfrz.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>asaxgevnnp.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>asdpeebotb.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>atkdltyyen.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>beboztfcme.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>bilnggbxgu.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>blpchvmhxx.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>cffffbcywc.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>dbnygxtwek.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>dhcndnuwta.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>diomeixhrg.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>drcyabprvt.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>dsjbknkujw.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>ecuvtoltue.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>ehtdtkmmli.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>eqnoqyfquo.mp4</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(submission_df))\nara = csvData.Label\n#ara\nsubmission_df.shape\n#for columns in submission_df.columns:\n#    print(columns)\npred = submission_df.label\n#print(pred)\ntempData = pd.DataFrame({'Name': csvData.Name ,'actual': ara, 'predicted' : pred})\ntempData.shape\nprint(tempData)\nloss = logLoss(tempData)\nprint(\"Loss: \")\nprint(loss)","execution_count":53,"outputs":[{"output_type":"stream","text":"22\n              Name  actual  predicted\n0   aagfhgtpmv.mp4       1   0.880910\n1   aapnvogymq.mp4       1   0.468023\n2   abofeumbvv.mp4       1   0.587143\n3   acxwigylke.mp4       1   0.974459\n4   akxoopqjqz.mp4       1   0.775321\n5   anpuvshzoo.mp4       0   0.219737\n6   apogckdfrz.mp4       1   0.738405\n7   asaxgevnnp.mp4       0   0.549570\n8   asdpeebotb.mp4       1   0.898248\n9   atkdltyyen.mp4       0   0.029246\n10  beboztfcme.mp4       0   0.210801\n11  bilnggbxgu.mp4       0   0.002897\n12  blpchvmhxx.mp4       1   0.991120\n13  cffffbcywc.mp4       1   0.570221\n14  dbnygxtwek.mp4       0   0.014031\n15  dhcndnuwta.mp4       0   0.386611\n16  diomeixhrg.mp4       1   0.995174\n17  drcyabprvt.mp4       0   0.008299\n18  dsjbknkujw.mp4       0   0.284073\n19  ecuvtoltue.mp4       1   0.977264\n20  ehtdtkmmli.mp4       0   0.142114\n21  eqnoqyfquo.mp4       0   0.008869\nLoss: \n0.228660034569695\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logLoss(data):\n    loss = 0.0\n    actual = data['actual']\n    predicted =  data['predicted']\n    total = len(actual)\n    for i in range(total):\n        loss += ((1.0  * actual[i]) * np.log(predicted[i]) + (1.0 * (1 - actual[i])) * np.log(1 - predicted[i]))\n    #print(loss)\n    loss = (-1.0 * loss) / (1.0 * total)\n    #print(\"Loss: \")\n    #print(loss)\n    return loss","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = np.arange(len(ara))\nbluebar = ara\norangebar = pred\n#print(bluebar)\n#print(orangebar)\nplt.figure(figsize = (25, 5))\nwidth = 0.2\nplt.bar(ind, bluebar , width, label='Ground truth')\nplt.bar(ind + width, orangebar, width, label='Prediction')\nnames = csvData.Name\nplt.ylabel('Here goes y-axis label')\nplt.title('Here goes title of the plot')\nplt.xticks(ind + width / 2,(submission_df.filename), rotation = 'vertical')\n\n# Finding the best position for legends and putting it\nplt.legend(loc='best')\nplt.show()","execution_count":77,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'name'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-77-8107ec6b3d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Here goes y-axis label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Here goes title of the plot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vertical'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Finding the best position for legends and putting it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'name'"]},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}