{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YoloV2_CVPRW2019_Face_Artifacts.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcvNiFhiVRYJ7HBJ0yQviW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmen/Deepfake-Challenge-Kaggle/blob/master/YoloV2_CVPRW2019_Face_Artifacts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOlC_qP6nVJ3",
        "colab_type": "code",
        "outputId": "59475177-91e9-4275-cada-330f777a8194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDAk4P2lMYwt",
        "colab_type": "code",
        "outputId": "a87f22eb-4a75-435f-ae00-6f6ad85a127b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone 'https://github.com/danmohaha/CVPRW2019_Face_Artifacts.git'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CVPRW2019_Face_Artifacts'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Total 106 (delta 0), reused 0 (delta 0), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (106/106), 73.19 MiB | 30.11 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKK5fkNsqOwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "3a000e0c-b2f1-44bc-d3d7-c2ec61dc8cb0"
      },
      "source": [
        "!pip install -U opencv-python==3.4.9.31"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-python==3.4.9.31\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/a6/a5965abd20edf570346dc98b0b4ce2cc71b6c9a551e05e4ca40cc08ef2c2/opencv_python-3.4.9.31-cp36-cp36m-manylinux1_x86_64.whl (28.2MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2MB 111kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.4.9.31) (1.18.2)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: opencv-python\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "Successfully installed opencv-python-3.4.9.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tHC9Tba0nOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "sys.path.insert(1,'CVPRW2019_Face_Artifacts/' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyqSvJqF0iZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from resolution_network import ResoNet\n",
        "from solver import Solver\n",
        "from easydict import EasyDict as edict\n",
        "import cv2, yaml, os, dlib\n",
        "from py_utils.vis import vis_im\n",
        "import numpy as np\n",
        "from py_utils.face_utils import lib\n",
        "from py_utils.vid_utils import proc_vid as pv\n",
        "import logging\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgi_IF8Sq37r",
        "colab_type": "code",
        "outputId": "252f0b7b-bf63-4205-c3ca-03047b5c9d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "print(cv2.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "3.4.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll5cSupgoJVz",
        "colab_type": "code",
        "outputId": "071df533-5072-49db-e228-4225ac48e00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_dir = \"drive/My Drive/test2/\"\n",
        "\n",
        "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
        "len(test_videos)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp078LrX38D8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b890dfb0-fbb5-493f-fd92-5908a1a30bc0"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "def load_mobilenetv2_224_075_detector(path):\n",
        "    input_tensor = Input(shape=(224, 224, 3))\n",
        "    output_tensor = MobileNetV2(weights=None, include_top=False, input_tensor=input_tensor, alpha=0.75).output\n",
        "    output_tensor = ZeroPadding2D()(output_tensor)\n",
        "    output_tensor = Conv2D(kernel_size=(3, 3), filters=5)(output_tensor)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    model.load_weights(path)\n",
        "    \n",
        "    return model\n",
        "\n",
        "mobilenetv2 = load_mobilenetv2_224_075_detector(\"drive/My Drive/YoloV2/facedetection-mobilenetv2-size224-alpha0.75.h5\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCm8-w234n2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts A:B aspect rate to B:A\n",
        "def transpose_shots(shots):\n",
        "    return [(shot[1], shot[0], shot[3], shot[2], shot[4]) for shot in shots]\n",
        "\n",
        "#That constant describe pieces for 16:9 images\n",
        "SHOTS = {\n",
        "    # fast less accurate\n",
        "    '2-16/9' : {\n",
        "        'aspect_ratio' : 16/9,\n",
        "        'shots' : [\n",
        "             (0, 0, 9/16, 1, 1),\n",
        "             (7/16, 0, 9/16, 1, 1)\n",
        "        ]\n",
        "    },\n",
        "    # slower more accurate\n",
        "    '10-16/9' : {\n",
        "        'aspect_ratio' : 16/9,\n",
        "        'shots' : [\n",
        "             (0, 0, 9/16, 1, 1),\n",
        "             (7/16, 0, 9/16, 1, 1),\n",
        "             (0, 0, 5/16, 5/9, 0.5),\n",
        "             (0, 4/9, 5/16, 5/9, 0.5),\n",
        "             (11/48, 0, 5/16, 5/9, 0.5),\n",
        "             (11/48, 4/9, 5/16, 5/9, 0.5),\n",
        "             (22/48, 0, 5/16, 5/9, 0.5),\n",
        "             (22/48, 4/9, 5/16, 5/9, 0.5),\n",
        "             (11/16, 0, 5/16, 5/9, 0.5),\n",
        "             (11/16, 4/9, 5/16, 5/9, 0.5),\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 9:16 respectively\n",
        "SHOTS_T = {\n",
        "    '2-9/16' : {\n",
        "        'aspect_ratio' : 9/16,\n",
        "        'shots' : transpose_shots(SHOTS['2-16/9']['shots'])\n",
        "    },\n",
        "    '10-9/16' : {\n",
        "        'aspect_ratio' : 9/16,\n",
        "        'shots' : transpose_shots(SHOTS['10-16/9']['shots'])\n",
        "    }\n",
        "}\n",
        "\n",
        "def r(x):\n",
        "    return int(round(x))\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (np.exp(-x) + 1)\n",
        "\n",
        "def non_max_suppression(boxes, p, iou_threshold):\n",
        "    if len(boxes) == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    indexes = np.argsort(p)\n",
        "    true_boxes_indexes = []\n",
        "\n",
        "    while len(indexes) > 0:\n",
        "        true_boxes_indexes.append(indexes[-1])\n",
        "\n",
        "        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n",
        "        iou = intersection / ((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]) + (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]) - intersection)\n",
        "\n",
        "        indexes = np.delete(indexes, -1)\n",
        "        indexes = np.delete(indexes, np.where(iou >= iou_threshold)[0])\n",
        "\n",
        "    return boxes[true_boxes_indexes]\n",
        "\n",
        "def union_suppression(boxes, threshold):\n",
        "    if len(boxes) == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    indexes = np.argsort((x2 - x1) * (y2 - y1))\n",
        "    result_boxes = []\n",
        "\n",
        "    while len(indexes) > 0:\n",
        "        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n",
        "        min_s = np.minimum((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]), (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]))\n",
        "        ioms = intersection / (min_s + 1e-9)\n",
        "        neighbours = np.where(ioms >= threshold)[0]\n",
        "        if len(neighbours) > 0:\n",
        "            result_boxes.append([min(np.min(x1[indexes[neighbours]]), x1[indexes[-1]]), min(np.min(y1[indexes[neighbours]]), y1[indexes[-1]]), max(np.max(x2[indexes[neighbours]]), x2[indexes[-1]]), max(np.max(y2[indexes[neighbours]]), y2[indexes[-1]])])\n",
        "        else:\n",
        "            result_boxes.append([x1[indexes[-1]], y1[indexes[-1]], x2[indexes[-1]], y2[indexes[-1]]])\n",
        "\n",
        "        indexes = np.delete(indexes, -1)\n",
        "        indexes = np.delete(indexes, neighbours)\n",
        "\n",
        "    return result_boxes\n",
        "\n",
        "class FaceDetector():\n",
        "    \"\"\"\n",
        "    That's API you can easily use to detect faces\n",
        "    \n",
        "    __init__ parameters:\n",
        "    -------------------------------\n",
        "    model - model to infer\n",
        "    shots - list of aspect ratios that images could be (described earlier)\n",
        "    image_size - model's input size (hardcoded for mobilenetv2)\n",
        "    grids - model's output size (hardcoded for mobilenetv2)\n",
        "    union_threshold - threshold for union of predicted boxes within multiple shots\n",
        "    iou_threshold - IOU threshold for non maximum suppression used to merge YOLO detected boxes for one shot,\n",
        "                    you do need to change this because there are one face per image as I can see from the samples\n",
        "    prob_threshold - probability threshold for YOLO algorithm, you can balance beetween precision and recall using this threshold\n",
        "    \n",
        "    detect parameters:\n",
        "    -------------------------------\n",
        "    frame - (1920, 1080, 3) or (1080, 1920, 3) RGB Image\n",
        "    returns: list of 4 element tuples (left corner x, left corner y, right corner x, right corner y) of detected boxes within [0, 1] range (see box draw code below)\n",
        "    \"\"\"\n",
        "    def __init__(self, model=mobilenetv2, shots=[SHOTS['10-16/9'], SHOTS_T['10-9/16']], image_size=224, grids=7, iou_threshold=0.1, union_threshold=0.1):\n",
        "        self.model = model\n",
        "        self.shots = shots\n",
        "        self.image_size = image_size\n",
        "        self.grids = grids\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.union_threshold = union_threshold\n",
        "        self.prob_threshold = 0.7\n",
        "        \n",
        "    \n",
        "    def detect(self, frame, threshold = 0.7):\n",
        "        original_frame_shape = frame.shape\n",
        "        self.prob_threshold = threshold\n",
        "        aspect_ratio = None\n",
        "        for shot in self.shots:\n",
        "            if abs(frame.shape[1] / frame.shape[0] - shot[\"aspect_ratio\"]) < 1e-9:\n",
        "                aspect_ratio = shot[\"aspect_ratio\"]\n",
        "                shots = shot\n",
        "        \n",
        "        assert aspect_ratio is not None\n",
        "        \n",
        "        c = min(frame.shape[0], frame.shape[1] / aspect_ratio)\n",
        "        slice_h_shift = r((frame.shape[0] - c) / 2)\n",
        "        slice_w_shift = r((frame.shape[1] - c * aspect_ratio) / 2)\n",
        "        if slice_w_shift != 0 and slice_h_shift == 0:\n",
        "            frame = frame[:, slice_w_shift:-slice_w_shift]\n",
        "        elif slice_w_shift == 0 and slice_h_shift != 0:\n",
        "            frame = frame[slice_h_shift:-slice_h_shift, :]\n",
        "\n",
        "        frames = []\n",
        "        for s in shots[\"shots\"]:\n",
        "            frames.append(cv2.resize(frame[r(s[1] * frame.shape[0]):r((s[1] + s[3]) * frame.shape[0]), r(s[0] * frame.shape[1]):r((s[0] + s[2]) * frame.shape[1])], (self.image_size, self.image_size), interpolation=cv2.INTER_NEAREST))\n",
        "        frames = np.array(frames)\n",
        "\n",
        "        predictions = self.model.predict(frames, batch_size=len(frames), verbose=0)\n",
        "\n",
        "        boxes = []\n",
        "        prob = []\n",
        "        shots = shots['shots']\n",
        "        for i in range(len(shots)):\n",
        "            slice_boxes = []\n",
        "            slice_prob = []\n",
        "            for j in range(predictions.shape[1]):\n",
        "                for k in range(predictions.shape[2]):\n",
        "                    p = sigmoid(predictions[i][j][k][4])\n",
        "                    if not(p is None) and p > self.prob_threshold:\n",
        "                        px = sigmoid(predictions[i][j][k][0])\n",
        "                        py = sigmoid(predictions[i][j][k][1])\n",
        "                        pw = min(math.exp(predictions[i][j][k][2] / self.grids), self.grids)\n",
        "                        ph = min(math.exp(predictions[i][j][k][3] / self.grids), self.grids)\n",
        "                        if not(px is None) and not(py is None) and not(pw is None) and not(ph is None) and pw > 1e-9 and ph > 1e-9:\n",
        "                            cx = (px + j) / self.grids\n",
        "                            cy = (py + k) / self.grids\n",
        "                            wx = pw / self.grids\n",
        "                            wy = ph / self.grids\n",
        "                            if wx <= shots[i][4] and wy <= shots[i][4]:\n",
        "                                lx = min(max(cx - wx / 2, 0), 1)\n",
        "                                ly = min(max(cy - wy / 2, 0), 1)\n",
        "                                rx = min(max(cx + wx / 2, 0), 1)\n",
        "                                ry = min(max(cy + wy / 2, 0), 1)\n",
        "\n",
        "                                lx *= shots[i][2]\n",
        "                                ly *= shots[i][3]\n",
        "                                rx *= shots[i][2]\n",
        "                                ry *= shots[i][3]\n",
        "\n",
        "                                lx += shots[i][0]\n",
        "                                ly += shots[i][1]\n",
        "                                rx += shots[i][0]\n",
        "                                ry += shots[i][1]\n",
        "\n",
        "                                slice_boxes.append([lx, ly, rx, ry])\n",
        "                                slice_prob.append(p)\n",
        "\n",
        "            slice_boxes = np.array(slice_boxes)\n",
        "            slice_prob = np.array(slice_prob)\n",
        "\n",
        "            slice_boxes = non_max_suppression(slice_boxes, slice_prob, self.iou_threshold)\n",
        "\n",
        "            for sb in slice_boxes:\n",
        "                boxes.append(sb)\n",
        "\n",
        "\n",
        "        boxes = np.array(boxes)\n",
        "        boxes = union_suppression(boxes, self.union_threshold)\n",
        "\n",
        "        for i in range(len(boxes)):\n",
        "            boxes[i][0] /= original_frame_shape[1] / frame.shape[1]\n",
        "            boxes[i][1] /= original_frame_shape[0] / frame.shape[0]\n",
        "            boxes[i][2] /= original_frame_shape[1] / frame.shape[1]\n",
        "            boxes[i][3] /= original_frame_shape[0] / frame.shape[0]\n",
        "\n",
        "            boxes[i][0] += slice_w_shift / original_frame_shape[1]\n",
        "            boxes[i][1] += slice_h_shift / original_frame_shape[0]\n",
        "            boxes[i][2] += slice_w_shift / original_frame_shape[1]\n",
        "            boxes[i][3] += slice_h_shift / original_frame_shape[0]\n",
        "\n",
        "        return list(boxes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRZ9mqSj4twk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_boxes_points(boxes, frame_shape):\n",
        "    result = []\n",
        "    for box in boxes:\n",
        "        lx = int(round(box[0] * frame_shape[1]))\n",
        "        ly = int(round(box[1] * frame_shape[0]))\n",
        "        rx = int(round(box[2] * frame_shape[1]))\n",
        "        ry = int(round(box[3] * frame_shape[0]))\n",
        "        result.append((lx, ly, rx, ry))\n",
        "    return result "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgdpjDGb4wry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yolo_model = FaceDetector()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bu259o9GROg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
        "    h, w = img.shape[:2]\n",
        "    if w > h:\n",
        "        h = h * size // w\n",
        "        w = size\n",
        "    else:\n",
        "        w = w * size // h\n",
        "        h = size\n",
        "\n",
        "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def make_square_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    size = max(h, w)\n",
        "    t = 0\n",
        "    b = size - h\n",
        "    l = 0\n",
        "    r = size - w\n",
        "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDq_hlFvnjyj",
        "colab_type": "text"
      },
      "source": [
        "Load model weight and Face Detector weights.\n",
        "Face Detector is **dlib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ07F5GB2cR0",
        "colab_type": "code",
        "outputId": "6148a09f-fc12-4283-f2ae-da214637dd0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "cfg_file = 'CVPRW2019_Face_Artifacts/cfgs/res50.yml'\n",
        "with open(cfg_file, 'r') as f:\n",
        "    cfg = edict(yaml.load(f))\n",
        "sample_num = 10\n",
        "\n",
        "print(cfg)\n",
        "\n",
        "front_face_detector = dlib.get_frontal_face_detector()\n",
        "lmark_predictor = dlib.shape_predictor('CVPRW2019_Face_Artifacts/dlib_model/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "tfconfig = tf.ConfigProto(allow_soft_placement=True)\n",
        "tfconfig.gpu_options.allow_growth=True\n",
        "# init session\n",
        "sess = tf.Session(config=tfconfig)\n",
        "# Build network\n",
        "reso_net = ResoNet(cfg=cfg, is_train=False)\n",
        "reso_net.build()\n",
        "# Build solver\n",
        "solver = Solver(sess=sess, cfg=cfg, net=reso_net)\n",
        "solver.init()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'BASE_NETWORK': 'res50', 'PRETRAINED_MODELS': '', 'IMG_SIZE': [224, 224, 3], 'PIXEL_MEAN': [103.939, 116.779, 123.68], 'NUM_CLASSES': 2, 'BKG_LABEL': 0, 'CLASS_WEIGHTS': 1, 'SUMMARY_DIR': 'summary', 'MODEL_DIR_PREFIX': 'ckpt', 'MODEL_NAME': 'model', 'TRAIN': {'LEARNING_RATE': 0.001, 'DECAY_RATE': 0.95, 'NUM_EPOCH': 20, 'BETA': 0.001, 'BATCH_SIZE': 64, 'NEG_HARD_MINING': 1.0, 'POS_HARD_MINING': 1.0}, 'TEST': {'BATCH_SIZE': 8}}\n",
            "WARNING:tensorflow:From CVPRW2019_Face_Artifacts/resolution_network.py:36: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From CVPRW2019_Face_Artifacts/solver.py:35: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From CVPRW2019_Face_Artifacts/solver.py:35: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From CVPRW2019_Face_Artifacts/solver.py:49: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/CVPRW2019_Face_Artifacts/ckpt_res50/model\n",
            "Loading checkpoint /content/CVPRW2019_Face_Artifacts/ckpt_res50/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqy1jpb8Dyoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_num = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e_0SuOX3OHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def im_test(im):\n",
        "    print(\"===== START SINGLE FRAME ===\")\n",
        "    start_im = time.time()\n",
        "\n",
        "    # face_info = lib.align(im[:, :, (2,1,0)], front_face_detector, lmark_predictor)\n",
        "    yolo_boxes = yolo_model.detect(im)\n",
        "    yb = get_boxes_points(yolo_boxes, im.shape)\n",
        "\n",
        "    print(\"LENGTH - \", len(yb))\n",
        "\n",
        "    # Samples\n",
        "    if len(yb) == 0:\n",
        "        logging.warning('No faces are detected.')\n",
        "        prob = -1  # we ignore this case\n",
        "    else:\n",
        "        # Check how many faces in an image\n",
        "        logging.info('{} faces are detected.'.format(len(yb)))\n",
        "        max_prob = -1\n",
        "        face_count = 0\n",
        "        # If one face is fake, the image is fake\n",
        "        for b in yb:\n",
        "            lx, ly, rx, ry = b\n",
        "            img_crop = im[ly:ry, lx:rx]\n",
        "            # print(img_crop.shape)\n",
        "            # plt.figure()\n",
        "            # plt.imshow(img_crop)\n",
        "            # plt.show()\n",
        "\n",
        "            rois = []\n",
        "            # for i in range(1): #Sampling same image multiple times and taking mean\n",
        "            #     resized_face = isotropically_resize_image(img_crop, 224)\n",
        "            #     resized_face = make_square_image(resized_face)\n",
        "            #     rois.append(resized_face)\n",
        "            for i in range(1):\n",
        "                rois.append(cv2.resize(img_crop, tuple(cfg.IMG_SIZE[:2])))\n",
        "            \n",
        "            vis_im(rois, 'vis{}.jpg'.format(face_count))\n",
        "            face_count += 1\n",
        "            \n",
        "            # for i in rois:\n",
        "            #     plt.figure()\n",
        "            #     plt.imshow(i)\n",
        "            #     plt.show()\n",
        "                \n",
        "            print(rois[0].shape)\n",
        "\n",
        "            prob = solver.test(rois)\n",
        "            prob = np.mean(np.sort(prob[:, 0])[np.round(sample_num / 2).astype(int):])\n",
        "            print(\"------>\",prob)\n",
        "            if prob >= max_prob:\n",
        "                max_prob = prob\n",
        "        prob = max_prob\n",
        "\n",
        "    end_im = time.time() - start_im\n",
        "    print(\"Single Frame time %f sec\" % (end_im))\n",
        "\n",
        "    return prob\n",
        "\n",
        "\n",
        "def run(input_dir):\n",
        "    logging.basicConfig(filename='run.log', filemode='w', format='[%(asctime)s - %(levelname)s] %(message)s',\n",
        "                        level=logging.INFO)\n",
        "\n",
        "    f_list = os.listdir(input_dir)\n",
        "    prob_list = []\n",
        "    for f_name in f_list:\n",
        "        start_time = time.time()\n",
        "        # Parse video\n",
        "        f_path = os.path.join(input_dir, f_name)\n",
        "        print('Testing: ' + f_path)\n",
        "        logging.info('Testing: ' + f_path)\n",
        "        suffix = f_path.split('.')[-1]\n",
        "        if suffix.lower() in ['jpg', 'png', 'jpeg', 'bmp', 'tif', 'nef', 'raf']:\n",
        "            im = cv2.imread(f_path)\n",
        "            if im is None:\n",
        "                prob = -1\n",
        "            else:\n",
        "                prob = im_test(im)\n",
        "\n",
        "        elif suffix.lower() in ['mp4', 'avi', 'mov']:\n",
        "            # Parse video\n",
        "            imgs, frame_num, fps, width, height = pv.parse_vid(f_path)\n",
        "            probs = []\n",
        "            for fid, im in enumerate(imgs):\n",
        "                logging.info('Frame ' + str(fid))\n",
        "                prob = im_test(im)\n",
        "                if prob == -1:\n",
        "                    continue\n",
        "                probs.append(prob)\n",
        "\n",
        "            # Remove opt out frames\n",
        "            if probs is []:\n",
        "                prob = -1\n",
        "            else:\n",
        "                prob = np.mean(sorted(probs, reverse=True)[:int(frame_num / 3)])\n",
        "\n",
        "        logging.info('Prob = ' + str(prob))\n",
        "        prob_list.append(prob)\n",
        "        print('Prob: ' + str(prob))\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"Elapsed %f sec\" % (elapsed))\n",
        "\n",
        "    sess.close()\n",
        "    return prob_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIHClIkXc5-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f9056354-e208-4d2b-d1d0-64e2f11bcbea"
      },
      "source": [
        "image = cv2.imread(\"00019.jpg\")\n",
        "res = im_test(image)\n",
        "print(res)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== START SINGLE FRAME ===\n",
            "LENGTH -  1\n",
            "(224, 224, 3)\n",
            "------> 0.0039105755\n",
            "Single Frame time 0.034926 sec\n",
            "0.0039105755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWomB7PN4SZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = run(test_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxDGUW2m4xBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run(\"drive/My Drive/test2\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}